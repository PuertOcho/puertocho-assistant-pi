# 🧠 Configuración de Entrenamiento del Modelo "Puertocho"
# Optimizada para Google Cloud T4

# Configuración del modelo
model:
  name: "puertocho_v1"
  architecture: "mel_stft_1d_cnn"  # Arquitectura openWakeWord
  sample_rate: 16000               # Tasa de muestreo estándar
  frame_length: 1280               # 80ms @ 16kHz (compatible con openWakeWord)
  num_mel_features: 32             # Número de características mel
  num_classes: 1                   # Clasificación binaria (Puertocho vs No-Puertocho)

# Configuración de entrenamiento
training:
  batch_size: 8                    # Optimizado para T4 16GB
  learning_rate: 1e-4              # Learning rate inicial
  epochs: 100                      # Máximo número de épocas
  early_stopping_patience: 10     # Parar si no mejora en 10 épocas
  validation_split: 0.2            # 20% para validación
  
  # Directorios
  models_dir: "models"
  logs_dir: "logs"
  
  # Guardado de modelos
  save_every_n_epochs: 5           # Guardar checkpoint cada 5 épocas
  save_best_only: true             # Solo guardar el mejor modelo

# Configuración de datos
data:
  base_dir: "data"
  positive_dir: "data/positive"    # Muestras de "Puertocho"
  negative_dir: "data/negative"    # Muestras de ruido/otras palabras
  
  # Métricas objetivo
  target_fpr: 0.5                  # False positives por hora (máximo)
  target_fnr: 0.05                 # False negatives (5% máximo)
  
  # Preprocesamiento de audio
  normalize_audio: true
  apply_augmentation: true         # Activar data augmentation
  
  # Data augmentation
  augmentation:
    add_noise: true                # Añadir ruido blanco
    noise_level: 0.005             # Nivel de ruido (0.5%)
    time_stretch: true             # Cambiar velocidad
    time_stretch_range: [0.9, 1.1] # Rango de cambio de velocidad
    pitch_shift: true              # Cambiar tono
    pitch_shift_range: [-2, 2]     # Rango en semitonos

# Configuración de optimización
optimization:
  # Optimizador
  optimizer: "adamw"
  weight_decay: 0.01
  
  # Mixed precision training (para T4)
  use_amp: true                    # Activar Automatic Mixed Precision
  
  # Gradient clipping
  gradient_clip_norm: 1.0
  
  # Learning rate scheduling
  scheduler: "onecycle"
  warmup_steps: 500                # Pasos de warmup
  
  # Loss function
  loss_function: "bce_with_logits" # Binary Cross Entropy con logits
  class_weight: null               # Auto-balance si hay desbalance

# Configuración de validación y métricas
validation:
  # Métricas a calcular
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1_score"
    - "auc_roc"
  
  # Threshold para clasificación
  classification_threshold: 0.5
  
  # Validación durante entrenamiento
  validate_every_n_epochs: 1      # Validar cada época
  
# Configuración de logging y monitoreo
logging:
  # Nivel de logging
  level: "INFO"
  
  # TensorBoard
  use_tensorboard: true
  tensorboard_log_dir: "logs/tensorboard"
  
  # Weights & Biases (opcional)
  use_wandb: false
  wandb_project: "puertocho-wakeword"
  wandb_entity: null               # Tu username de W&B
  
  # Logging de métricas
  log_every_n_steps: 100          # Log cada 100 steps
  log_images: false               # No necesario para audio
  
# Configuración específica para hardware
hardware:
  # GPU
  gpu_memory_fraction: 0.9        # Usar 90% de memoria GPU
  allow_growth: true              # Permitir crecimiento de memoria
  
  # CPU
  num_workers: 4                  # Workers para DataLoader
  pin_memory: true               # Pin memory para GPU transfer
  
  # Optimizaciones para T4
  use_cuda_graphs: false         # Experimental, puede causar problemas
  compile_model: false           # PyTorch 2.0 compile (experimental)

# Configuración de reproducibilidad
reproducibility:
  seed: 42                       # Semilla aleatoria
  deterministic: false           # Deterministic ops (más lento)
  benchmark: true               # CuDNN benchmark (más rápido)

# Configuración específica de openWakeWord
openwakeword:
  # Arquitectura del modelo
  model_architecture:
    type: "1d_cnn"               # Tipo de arquitectura
    num_layers: 4                # Número de capas
    hidden_size: 128             # Tamaño oculto
    dropout: 0.3                 # Dropout rate
    
  # Configuración de entrada
  input_features:
    type: "mel_spectrogram"      # Tipo de características
    n_mels: 32                   # Número de bandas mel
    hop_length: 320              # Hop length para STFT
    win_length: 640              # Window length para STFT
    
  # Postprocesamiento
  postprocessing:
    smoothing_window: 5          # Ventana de suavizado
    threshold_adjustment: 0.0    # Ajuste de threshold
    
# Configuración de exportación
export:
  # Formato ONNX
  export_onnx: true
  onnx_opset: 11                 # Versión del opset ONNX
  onnx_dynamic_axes: true        # Ejes dinámicos para batch size
  
  # Optimizaciones ONNX
  onnx_optimize: true
  onnx_fp16: false               # FP16 puede causar problemas
  
  # TensorRT (para deployment)
  export_tensorrt: false        # Experimental
  
# Configuración de pruebas
testing:
  # Dataset de prueba
  test_split: 0.1               # 10% para testing final
  
  # Métricas de evaluación final
  final_evaluation_metrics:
    - "accuracy"
    - "precision" 
    - "recall"
    - "f1_score"
    - "confusion_matrix"
    - "roc_curve"
    - "false_positive_rate_per_hour"
    - "false_negative_rate"
  
  # Pruebas de robustez
  robustness_tests:
    noise_levels: [0.01, 0.05, 0.1]  # Diferentes niveles de ruido
    volume_levels: [0.5, 1.0, 1.5]   # Diferentes volúmenes
    
# Notas de configuración
notes: |
  Configuración optimizada para:
  - Google Cloud T4 GPU (16GB VRAM)
  - Dataset ~2000 muestras positivas + ~10000 negativas
  - Objetivo: FPR < 0.5/hora, FNR < 5%
  - Tiempo estimado: 3-6 horas de entrenamiento
  
  Ajustes recomendados según resultados:
  - Si overfitting: aumentar dropout, reducir hidden_size
  - Si underfitting: aumentar hidden_size, más épocas
  - Si FPR alto: aumentar threshold o datos negativos
  - Si FNR alto: reducir threshold o más datos positivos 