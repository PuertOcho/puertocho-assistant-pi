# PROJECT_TRACKER.md

## Fase 1: Preparaci√≥n del entorno ‚úÖ COMPLETADA
- [x] A√±adir `openwakeword` y dependencias a `requirements.txt`
- [x] Instalar dependencias del sistema necesarias (ej: `libsndfile1`, `libasound2`, `libsndfile-dev`, `libspeexdsp-dev` si se usa supresi√≥n de ruido)
- [x] Configurar variables de entorno espec√≠ficas para openWakeWord
- [x] Documentar instalaci√≥n y dependencias en el README

## Fase 2: Integraci√≥n b√°sica de openWakeWord ‚úÖ COMPLETADA
- [x] Descargar modelos preentrenados de openWakeWord (`openwakeword.utils.download_models()`)
- [x] Crear clase/m√≥dulo para inicializar y gestionar el modelo de wake word
- [x] Implementar captura de audio en tiempo real (usar `sounddevice`, frames de 80ms multiples)
- [x] Integrar la predicci√≥n del modelo sobre frames de audio (16kHz, 16bit PCM)
- [x] Implementar sistema de buffering de audio eficiente
- [x] Probar detecci√≥n b√°sica de wake word y log de activaciones

## Fase 3: Integraci√≥n con l√≥gica del asistente ‚úÖ COMPLETADA
- [x] Adaptar la l√≥gica de activaci√≥n (sustituir Porcupine por openWakeWord en el flujo principal)
- [x] Integrar con el sistema de comandos (ejecuci√≥n tras detecci√≥n de wake word)
- [x] Mantener compatibilidad con LEDs y bot√≥n f√≠sico (GPIO)
- [x] Implementar gesti√≥n de estados (IDLE, LISTENING, PROCESSING)
- [x] A√±adir tests de integraci√≥n para la activaci√≥n y flujo de comandos

## Fase 4: Optimizaci√≥n y robustez ‚úÖ COMPLETADA
- [x] **Configuraci√≥n de par√°metros avanzados:**
  - [x] Ajustar umbral de activaci√≥n (threshold) seg√∫n entorno - Subido a 0.6 para evitar detecciones m√∫ltiples
  - [x] Configurar VAD (Voice Activity Detection) con `vad_threshold` - Implementado y configurable
  - [x] Implementar supresi√≥n de ruido Speex (`enable_speex_noise_suppression=True`) - Disponible por variable de entorno
- [ ] **Optimizaci√≥n de rendimiento:**
  - [ ] Medir latencia y consumo de CPU/RAM en Raspberry Pi
  - [ ] Optimizar tama√±o de frames (multiples de 80ms para eficiencia)
  - [ ] Configurar inference framework (ONNX vs TFLite seg√∫n hardware)
- [ ] **Mejoras de robustez:**
  - [ ] Implementar m√©tricas de rendimiento en tiempo real
  - [ ] A√±adir monitoreo de temperatura y throttling
  - [ ] Sistema de auto-ajuste de par√°metros seg√∫n condiciones
- [ ] Permitir selecci√≥n/cambio de modelo de wake word (personalizaci√≥n)
- [ ] Implementar fallback a modelos preentrenados si falla el personalizado

## Fase 5: Entrenamiento de modelo personalizado "Puertocho" üîÑ EN PROGRESO
- [ ] **Preparaci√≥n del entorno de entrenamiento:**
  - [ ] **OPCI√ìN RECOMENDADA: Google Cloud GPU T4** (basado en experiencia previa exitosa)
    - [ ] Configurar instancia con GPU T4 (similar a docs/FASE_3_IMPLEMENTACION_CLOUD.md)
    - [ ] Adaptar scripts de setup para openWakeWord en lugar de ChatterboxTTS
    - [ ] Configurar entorno: Python 3.11+, PyTorch, openWakeWord dependencies
    - [ ] Costo estimado: $10-15 para entrenamiento completo
  - [ ] **OPCI√ìN ALTERNATIVA: Google Colab** (gratuito pero con limitaciones)
    - [ ] Usar notebook oficial de openWakeWord
    - [ ] Considerar limitaciones de tiempo (12h) y sesiones cortadas
- [ ] **Preparaci√≥n de datos:**
  - [ ] Generar datos sint√©ticos positivos usando TTS
    - [ ] "Puertocho"
    - [ ] "Hola Puertocho" 
    - [ ] "Oye Puertocho"
    - [ ] Variaciones con diferentes voces y acentos (m√≠nimo varios miles)
  - [ ] Generar/descargar datos negativos (~30,000 horas recomendadas)
  - [ ] Subir dataset a Cloud Storage si usas Google Cloud
- [ ] **Entrenamiento:**
  - [ ] **Si usas Google Cloud T4:**
    - [ ] Adaptar scripts de entrenamiento existentes (train_spanish_*.py ‚Üí train_puertocho_*.py)
    - [ ] Configurar Wandb para monitoreo (modo offline funcional)
    - [ ] Usar configuraci√≥n optimizada para T4: batch_size=8, learning_rate=1e-4
  - [ ] **Si usas Google Colab:**
    - [ ] Usar notebook oficial de openWakeWord para entrenamiento
    - [ ] Configurar par√°metros de entrenamiento (√©pocas, batch size, etc.)
  - [ ] Entrenar modelo (tiempo estimado: <1 hora)
  - [ ] Validar rendimiento del modelo entrenado
- [ ] **Integraci√≥n del modelo personalizado:**
  - [ ] Descargar modelo `.onnx` o `.tflite` entrenado
  - [ ] Integrar en checkpoints del proyecto
  - [ ] Probar detecci√≥n con el modelo personalizado
  - [ ] Ajustar threshold espec√≠fico para "Puertocho"
- [ ] **Documentaci√≥n del proceso:**
  - [ ] Documentar proceso de entrenamiento paso a paso
  - [ ] Crear gu√≠a para futuras wake words personalizadas
  - [ ] Documentar troubleshooting y recomendaciones

## Fase 6: Validaci√≥n y despliegue
- [ ] **Testing exhaustivo:**
  - [ ] Probar en diferentes condiciones de ruido
  - [ ] Validar false-accept rate (<0.5/hora objetivo)
  - [ ] Validar false-reject rate (<5% objetivo)
  - [ ] Probar susurros y diferentes velocidades de habla
- [ ] **Optimizaci√≥n para producci√≥n:**
  - [ ] Validar funcionamiento en Raspberry Pi y otros entornos
  - [ ] Configurar logging y monitoreo de rendimiento
  - [ ] Implementar recuperaci√≥n ante errores
- [ ] **Documentaci√≥n final:**
  - [ ] Documentar troubleshooting y recomendaciones
  - [ ] Actualizar scripts de instalaci√≥n y verificaci√≥n para openWakeWord
  - [ ] Crear gu√≠a de usuario final

## Fase 7: Mejoras avanzadas (opcional)
- [ ] **Modelos verifier para voces espec√≠ficas:**
  - [ ] Entrenar modelo de segunda etapa para reconocer voz del usuario
  - [ ] Reducir false-accepts de otras voces
- [ ] **Soporte multilenguaje:**
  - [ ] Investigar TTS en espa√±ol para datos sint√©ticos
  - [ ] Experimentar con modelos base en otros idiomas
- [ ] **Integraci√≥n con servicios cloud:**
  - [ ] Backup a servicios de transcripci√≥n remotos
  - [ ] Telemetr√≠a y mejora continua

---

## Notas t√©cnicas y mejores pr√°cticas

### **Configuraci√≥n recomendada openWakeWord:**
```python
from openwakeword.model import Model

# Configuraci√≥n optimizada para Raspberry Pi
model = Model(
    wakeword_models=["checkpoints/puertocho.onnx"],  # Tu modelo personalizado
    inference_framework="onnx",  # M√°s eficiente en RPi
    vad_threshold=0.5,  # Ajustar seg√∫n entorno
    enable_speex_noise_suppression=True  # Para ambientes ruidosos
)
```

### **Par√°metros de audio recomendados:**
- **Frecuencia de muestreo:** 16kHz (requerido)
- **Resoluci√≥n:** 16-bit PCM
- **Canales:** Mono (1 canal)
- **Frame size:** M√∫ltiplos de 80ms (1280 samples @ 16kHz)
- **Buffer size:** 3-5 frames para latencia vs eficiencia

### **Thresholds t√≠picos por modelo:**
- Modelos preentrenados: 0.5 (default)
- Modelos personalizados: 0.3-0.8 (ajustar seg√∫n pruebas)
- Ambientes ruidosos: threshold m√°s alto
- Requisitos de baja latencia: threshold m√°s bajo

---

## Recursos y enlaces
- [Repositorio oficial openWakeWord](https://github.com/dscripka/openWakeWord)
- [Ejemplo de uso b√°sico](https://github.com/dscripka/openWakeWord#usage)
- [Recomendaciones de uso y par√°metros](https://github.com/dscripka/openWakeWord#recommendations-for-usage)
- [Entrenamiento de modelos personalizados](https://github.com/dscripka/openWakeWord#training-new-models)
- [Notebook de entrenamiento en Colab](https://github.com/dscripka/openWakeWord/blob/main/notebooks/)
- [Repositorio de generaci√≥n de datos sint√©ticos](https://github.com/dscripka/synthetic_speech_dataset_generation)
- [Modelos de la comunidad Home Assistant](https://github.com/fwartner/home-assistant-wakewords-collection)

---

Este archivo debe actualizarse conforme avance el desarrollo, marcando tareas completadas y a√±adiendo nuevas seg√∫n necesidades del proyecto. 